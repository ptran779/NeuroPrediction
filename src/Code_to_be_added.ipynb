{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imputing the missing columns\n"
      ],
      "metadata": {
        "id": "scwf0q28j32o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0N8GpLMj0GV"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import ast  # Module to safely evaluate literal strings as Python expressions\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "column_to_impute = 'pre_morph_embeddings'\n",
        "\n",
        "data[column_to_impute].fillna(0, inplace=True)\n",
        "\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "PDVOQtNOkavE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_columns = ['compartment', 'pre_brain_area','post_brain_area']\n",
        "\n",
        "# Apply label encoding to each categorical column\n",
        "for column in categorical_columns:\n",
        "    data[column] = label_encoder.fit_transform(data[column])\n",
        "\n",
        "# Display the updated DataFrame\n",
        "data.info()"
      ],
      "metadata": {
        "id": "iD2B-vkTkZ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean Distances and interaction terms"
      ],
      "metadata": {
        "id": "LuFHO9xzkkIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Eulidean distances between the axonal and dendritic coordinates\n",
        "\n",
        "\n",
        "\n",
        "# Calculate Euclidean distances and create a new column 'euclidean_distance'\n",
        "data['euclidean_distance'] = np.sqrt(\n",
        "    (data['axonal_coor_x'] - data['dendritic_coor_x'])**2 +\n",
        "    (data['axonal_coor_y'] - data['dendritic_coor_y'])**2 +\n",
        "    (data['axonal_coor_z'] - data['dendritic_coor_z'])**2\n",
        ")\n",
        "\n",
        "# The 'euclidean_distance' column now contains the Euclidean distances\n",
        "\n",
        "# Calculate Euclidean distances between axonal and dendritic coordinates for both pre and post neurons\n",
        "data['axonal_distance_pre'] = np.sqrt((data['axonal_coor_x'] - data['dendritic_coor_x'])**2 + (data['axonal_coor_y'] - data['dendritic_coor_y'])**2)\n",
        "data['axonal_distance_post'] = np.sqrt((data['axonal_coor_x'] - data['dendritic_coor_x'])**2 + (data['axonal_coor_y'] - data['dendritic_coor_y'])**2)\n",
        "\n",
        "data['dendritic_distance_pre'] = np.sqrt((data['dendritic_coor_x'] - data['axonal_coor_x'])**2 + (data['dendritic_coor_y'] - data['axonal_coor_y'])**2)\n",
        "data['dendritic_distance_post'] = np.sqrt((data['dendritic_coor_x'] - data['axonal_coor_x'])**2 + (data['dendritic_coor_y'] - data['axonal_coor_y'])**2)\n",
        "\n",
        "# Combine these distances with Euclidean distances between readout locations\n",
        "data['euclidean_distance_pre'] = np.sqrt((data['pre_rf_x'] - data['post_rf_x'])**2 + (data['pre_rf_y'] - data['post_rf_y'])**2)\n",
        "data['euclidean_distance_post'] = np.sqrt((data['pre_rf_x'] - data['post_rf_x'])**2 + (data['pre_rf_y'] - data['post_rf_y'])**2)\n",
        "\n",
        "# Dataset contains both structural (axonal and dendritic distances) and spatial (Euclidean distances) features.\n",
        "\n",
        "\n",
        "# Calculate the relative positions of post-synaptic neurons with respect to pre-synaptic neurons\n",
        "data['relative_position_x'] = data['post_rf_x'] - data['pre_rf_x']\n",
        "data['relative_position_y'] = data['post_rf_y'] - data['pre_rf_y']\n",
        "\n",
        "# dataset contains \"relative_position_x\" and \"relative_position_y\" features\n",
        "\n",
        "# Create binary directional features\n",
        "data['is_left'] = (data['relative_position_x'] < 0).astype(int)\n",
        "data['is_right'] = (data['relative_position_x'] > 0).astype(int)\n",
        "data['is_above'] = (data['relative_position_y'] < 0).astype(int)\n",
        "data['is_below'] = (data['relative_position_y'] > 0).astype(int)\n",
        "\n",
        "# Dataset contains binary directional features\n",
        "\n",
        "# Create interaction terms between rf_x and rf_y for pre-synaptic neurons\n",
        "data['pre_rf_x_rf_y_interaction'] = data['pre_rf_x'] * data['pre_rf_y']\n",
        "\n",
        "# Create interaction terms between rf_x and rf_y for post-synaptic neurons\n",
        "data['post_rf_x_rf_y_interaction'] = data['post_rf_x'] * data['post_rf_y']\n",
        "\n",
        "# The dataset now contains interaction terms that capture relationships in visual space"
      ],
      "metadata": {
        "id": "rmGDOF1KkpeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the morph embeddings"
      ],
      "metadata": {
        "id": "GFshCZdWkwm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
        "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
        "\n",
        "def flatten_arrays(arrays):\n",
        "    return [item for sublist in arrays for item in sublist]\n",
        "\n",
        "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(flatten_arrays)\n",
        "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(flatten_arrays)\n",
        "data['pre_morph_embeddings']"
      ],
      "metadata": {
        "id": "MEptYlFtk5ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA on morph embeddings"
      ],
      "metadata": {
        "id": "Oa85v8GRlTE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Ensure all arrays in 'pre_morph_embeddings' and 'post_morph_embeddings' have the same size\n",
        "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(lambda x: np.array(x) if len(x) == 32 else np.array([0.0] * 32))\n",
        "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(lambda x: np.array(x) if len(x) == 32 else np.array([0.0] * 32))\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "data['pre_morph_embeddings'] = list(pca.fit_transform(np.vstack(data['pre_morph_embeddings'])))\n",
        "data['post_morph_embeddings'] = list(pca.fit_transform(np.vstack(data['post_morph_embeddings'])))\n",
        "data['post_morph_embeddings']"
      ],
      "metadata": {
        "id": "mYr7FdcqlVIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA on feature weights"
      ],
      "metadata": {
        "id": "Mi1Rd-9uln_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "data['pre_feature_weights'] = data['pre_feature_weights'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
        "data['post_feature_weights'] = data['post_feature_weights'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
        "\n",
        "\n",
        "def flatten_arrays(arrays):\n",
        "    return [item for sublist in arrays for item in sublist]\n",
        "\n",
        "data['pre_feature_weights'] = data['pre_feature_weights'].apply(flatten_arrays)\n",
        "data['post_feature_weights'] = data['post_feature_weights'].apply(flatten_arrays)\n",
        "\n",
        "\n",
        "# Apply PCApost_feature_weights\n",
        "pca = PCA(n_components=2)\n",
        "data['pre_feature_weights'] = list(pca.fit_transform(np.vstack(data['pre_feature_weights'])))\n",
        "data['post_feature_weights'] = list(pca.fit_transform(np.vstack(data['post_feature_weights'])))\n",
        "\n",
        "\n",
        "\n",
        "# # Extract feature weight columns\n",
        "# feature_weight_columns = ['pre_feature_weights', 'post_feature_weights']\n",
        "\n",
        "# # Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# data[feature_weight_columns] = scaler.fit_transform(data[feature_weight_columns])\n",
        "\n",
        "# # Create a PCA object\n",
        "# pca = PCA(n_components=2)\n",
        "# data_pca = pca.fit_transform(data[feature_weight_columns])\n"
      ],
      "metadata": {
        "id": "pQA_qNy3lqIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kurtosis, skew, stats - feature weights"
      ],
      "metadata": {
        "id": "MUGBCtjilwfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "data['pre_feature_weights_sum'] = data['pre_feature_weights'].apply(lambda x: sum(x))\n",
        "data['post_feature_weights_sum'] = data['post_feature_weights'].apply(lambda x: sum(x))\n",
        "data['feature_weights_difference'] = abs(data['pre_feature_weights_sum'] - data['post_feature_weights_sum'])\n",
        "\n",
        "data['pre_feature_weights_variance'] = data['pre_feature_weights'].apply(lambda x: np.var(x))\n",
        "data['post_feature_weights_variance'] = data['post_feature_weights'].apply(lambda x: np.var(x))\n",
        "\n",
        "\n",
        "data['pre_feature_weights_skewness'] = data['pre_feature_weights'].apply(lambda x: skew(x))\n",
        "data['post_feature_weights_skewness'] = data['post_feature_weights'].apply(lambda x: skew(x))\n",
        "data['pre_feature_weights_kurtosis'] = data['pre_feature_weights'].apply(lambda x: kurtosis(x))\n",
        "data['post_feature_weights_kurtosis'] = data['post_feature_weights'].apply(lambda x: kurtosis(x))"
      ],
      "metadata": {
        "id": "PK7yZ-Oflxdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skew, stats, kurt for Morph embeddings"
      ],
      "metadata": {
        "id": "91XHyw74mA8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sum the morphological embeddings for pre and post neurons\n",
        "data['pre_morph_embeddings_sum'] = data['pre_morph_embeddings'].apply(lambda x: np.sum(x, axis=0))\n",
        "data['post_morph_embeddings_sum'] = data['post_morph_embeddings'].apply(lambda x: np.sum(x, axis=0))\n",
        "\n",
        "# Calculate the Euclidean distance between summed morphological embeddings\n",
        "data['morph_embeddings_distance'] = data.apply(lambda row: np.linalg.norm(row['pre_morph_embeddings_sum'] - row['post_morph_embeddings_sum']), axis=1)\n",
        "\n",
        "# Calculate the variance of morphological embeddings\n",
        "data['pre_morph_embeddings_variance'] = data['pre_morph_embeddings'].apply(lambda x: np.var(x, axis=0))\n",
        "data['post_morph_embeddings_variance'] = data['post_morph_embeddings'].apply(lambda x: np.var(x, axis=0))\n",
        "\n",
        "# Calculate the skewness and kurtosis of morphological embeddings\n",
        "data['pre_morph_embeddings_skewness'] = data['pre_morph_embeddings'].apply(lambda x: skew(x, axis=0))\n",
        "data['post_morph_embeddings_skewness'] = data['post_morph_embeddings'].apply(lambda x: skew(x, axis=0))\n",
        "data['pre_morph_embeddings_kurtosis'] = data['pre_morph_embeddings'].apply(lambda x: kurtosis(x, axis=0))\n",
        "data['post_morph_embeddings_kurtosis'] = data['post_morph_embeddings'].apply(lambda x: kurtosis(x, axis=0))\n"
      ],
      "metadata": {
        "id": "TYhzYtBAl4bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity"
      ],
      "metadata": {
        "id": "BfNAIkh6mKwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cosine similarity function\n",
        "def row_feature_similarity(row):\n",
        "    pre = row[\"pre_feature_weights\"]\n",
        "    post = row[\"post_feature_weights\"]\n",
        "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))"
      ],
      "metadata": {
        "id": "SFOLqmdLmMIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA with Explained variance on whole data set"
      ],
      "metadata": {
        "id": "opwK4ynrmd8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Exclude non-numeric and non-categorical columns from PCA\n",
        "exclude_cols = ['ID',\n",
        "                'pre_feature_weights',\n",
        "                'post_feature_weights',\n",
        "                'pre_morph_embeddings',\n",
        "                'post_morph_embeddings',\n",
        "                'projection_group',\n",
        "                'connected',\n",
        "                'is_left',\n",
        "                'is_right',\n",
        "                'is_above',\n",
        "                'is_below'\n",
        "               ]\n",
        "\n",
        "# Select only numeric columns for PCA\n",
        "numeric_columns = [col for col in data.columns if col not in exclude_cols]\n",
        "\n",
        "# Standardize the data (if needed)\n",
        "scaler = StandardScaler()\n",
        "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit PCA and transform the data\n",
        "pca_result = pca.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Cumulative explained variance\n",
        "cumulative_explained_variance = explained_variance.cumsum()\n",
        "\n",
        "# Visualize explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-', color='b')\n",
        "plt.title('Explained Variance with PCA')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3ubaKlHDmjX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Exclude non-numeric and non-categorical columns from the PCA\n",
        "exclude_cols = ['ID',\n",
        "                'pre_feature_weights',\n",
        "                'post_feature_weights',\n",
        "                'pre_morph_embeddings',\n",
        "                'post_morph_embeddings',\n",
        "                'projection_group',\n",
        "                'connected',\n",
        "                'is_left',\n",
        "                'is_right',\n",
        "                'is_above',\n",
        "                'is_below'\n",
        "               ]\n",
        "\n",
        "# Select only numeric columns for PCA\n",
        "numeric_columns = [col for col in data.columns if col not in exclude_cols]\n",
        "pca_num_of_components = 30\n",
        "\n",
        "# Standardize the data (if needed)\n",
        "scaler = StandardScaler()\n",
        "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Create a PCA object (you can specify the number of components)\n",
        "pca = PCA(n_components=pca_num_of_components)\n",
        "\n",
        "# Fit and transform the data\n",
        "pca_result = pca.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=[col for col in range(pca_num_of_components)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7-AHwtfmrpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K means clustering"
      ],
      "metadata": {
        "id": "KOFN3GKimvOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Select only numeric columns for KMeans clustering\n",
        "numeric_columns_for_clustering = [col for col in pca_df.columns if col != 'Cluster']\n",
        "\n",
        "num_clusters = 2\n",
        "\n",
        "# Create a KMeans object\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "# Fit the model to the PCA results\n",
        "kmeans.fit(pca_df[numeric_columns_for_clustering])\n",
        "\n",
        "# Add the cluster labels to the PCA DataFrame\n",
        "pca_df['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Visualize the clusters in the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster in range(num_clusters):\n",
        "    cluster_data = pca_df[pca_df['Cluster'] == cluster]\n",
        "    plt.scatter(cluster_data[0], cluster_data[1], label=f'Cluster {cluster}', alpha=0.7)\n",
        "\n",
        "plt.title('K-means Clustering after PCA')\n",
        "plt.xlabel('Principal Component 1 (PC1)')\n",
        "plt.ylabel('Principal Component 2 (PC2)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HIg4NHzMmwnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}