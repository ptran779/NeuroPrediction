{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "#Authour: Phong Tran & Benedict Andam\n",
    "#Team name: Raven One"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36a78251c36fd36e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### This was copy from all other script instead of being properly organized for submission. Hence, there may be a lot of file/code has incorrect path/organization. Please use this link to go to the correct source on Github\n",
    "\n",
    "https://github.com/ptran779/NeuroPrediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e532b16b6c8bc3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "960dfec9c87efdf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5d308becf62984f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prepare the data\n",
    "# read in data\n",
    "data = pd.read_csv(\"../resource/train_data.csv\")\n",
    "data2 = pd.read_csv(\"../resource/train_data2.csv\")\n",
    "\n",
    "# load in additional features for each neuron\n",
    "feature_weights = pd.read_csv(\"../resource/feature_weights.csv\")\n",
    "morph_embeddings = pd.read_csv(\"../resource/morph_embeddings.csv\")\n",
    "\n",
    "# join all feature_weights\n",
    "feature_weights[\"feature_weights\"] = (\n",
    "    feature_weights.filter(regex=\"feature_weight_\")\n",
    "    .sort_index(axis=1)\n",
    "    .apply(lambda x: np.array(x), axis=1)\n",
    ")\n",
    "# delete the feature_weight_i columns\n",
    "feature_weights.drop(\n",
    "    feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
    ")\n",
    "\n",
    "# join all morph_embed_i columns into a single np.array column\n",
    "morph_embeddings[\"morph_embeddings\"] = (\n",
    "    morph_embeddings.filter(regex=\"morph_emb_\")\n",
    "    .sort_index(axis=1)\n",
    "    .apply(lambda x: np.array(x), axis=1)\n",
    ")\n",
    "# delete the morph_embed_i columns\n",
    "morph_embeddings.drop(\n",
    "    morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
    ")\n",
    "\n",
    "data = (\n",
    "    data.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "data2 = (\n",
    "    data2.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0625753eff9daa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reorganized data and remove all useless neuron -- write output to new file\n",
    "linkedDat = data[data['connected'] == True]\n",
    "print(\"Only {} out of {} are real sender\".format(linkedDat['pre_nucleus_id'].nunique(), data['pre_nucleus_id'].nunique()))\n",
    "print(\"Only {} out of {} are real receiver\".format(linkedDat['post_nucleus_id'].nunique(), data['post_nucleus_id'].nunique()))\n",
    "\n",
    "# obtain useful neuro related set (neuron that actually connect)\n",
    "preNeroLinked = set(linkedDat['pre_nucleus_id'].unique())\n",
    "postNeroLinked = set(linkedDat['post_nucleus_id'].unique())\n",
    "\n",
    "# down-sample to only point that contain 2 useful neuron\n",
    "usefullDat = data[(data['pre_nucleus_id']).isin(preNeroLinked) & (data['post_nucleus_id'].isin(postNeroLinked))]\n",
    "print(len(usefullDat))\n",
    "usefullDat.info()\n",
    "usefullDat.to_csv('data2.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0382eeae714aaf0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization of all neurons position and maybe synapses. WARN: expect lag for plot "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df172acfb22950a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# extract only connected neuron\n",
    "linkedDat = data[data['connected'] == True]\n",
    "\n",
    "pairs1 = set()\n",
    "pairs2 = {}\n",
    "c = 0\n",
    "for index, row in data.iterrows():\n",
    "    pairs1.add((row['pre_nucleus_x'], row['pre_nucleus_y'], row['pre_nucleus_z']))\n",
    "    pairs1.add((row['post_nucleus_x'], row['post_nucleus_y'], row['post_nucleus_z']))\n",
    "for index, row in linkedDat.iterrows():\n",
    "    tmp = (row['pre_nucleus_id'], row['post_nucleus_id'])\n",
    "    if tmp in pairs2:\n",
    "        c += 1\n",
    "    else:\n",
    "        pairs2[tmp] = (row['pre_nucleus_x'], row['post_nucleus_x'], row['pre_nucleus_y'], row['post_nucleus_y'],\n",
    "                       row['pre_nucleus_z'], row['post_nucleus_z'])\n",
    "\n",
    "\n",
    "def color_nodes(G):\n",
    "    \"\"\"Colors the nodes of a graph based on their output and input.\n",
    "\n",
    "    Args:\n",
    "        G: A networkx graph object.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping node IDs to colors.\n",
    "    \"\"\"\n",
    "\n",
    "    colors = []\n",
    "    for node in G.nodes():\n",
    "        if G.out_degree(node) > 0:\n",
    "            if G.in_degree(node) > 0:\n",
    "                colors.append(\"purple\")\n",
    "            else:\n",
    "                colors.append(\"red\")\n",
    "        else:\n",
    "            colors.append(\"blue\")\n",
    "    return colors\n",
    "\n",
    "\n",
    "# Map non-filter direct neuron\n",
    "G1 = nx.DiGraph()\n",
    "for i in pairs2:  # add all pre and post to map\n",
    "    G1.add_node(i[0], XY=(pairs2[i][0], pairs2[i][2]), XZ=(pairs2[i][0], pairs2[i][4]))\n",
    "    G1.add_node(i[1], XY=(pairs2[i][1], pairs2[i][3]), XZ=(pairs2[i][1], pairs2[i][5]))\n",
    "    G1.add_edge(i[0], i[1])\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for i in pairs1:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "    z.append(i[2])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x, y, color='pink')\n",
    "colors = color_nodes(G1)\n",
    "XY = nx.get_node_attributes(G1, 'XY')\n",
    "nx.draw(G1, pos=XY, node_color=colors, node_size=20)\n",
    "plt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "plt.axis(True)\n",
    "plt.xlabel(\"x (nm)\")\n",
    "plt.ylabel(\"y (nm)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x, y, color='pink')\n",
    "XY = nx.get_node_attributes(G1, 'XY')\n",
    "nx.draw_networkx_nodes(G1, pos=XY, node_color=colors, node_size=20)\n",
    "plt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "plt.axis(True)\n",
    "plt.xlabel(\"x (nm)\")\n",
    "plt.ylabel(\"y (nm)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "XY = nx.get_node_attributes(G1, 'XY')\n",
    "nx.draw_networkx_nodes(G1, pos=XY, node_color=colors, node_size=20)\n",
    "plt.scatter(linkedDat.dendritic_coor_x.values, linkedDat.dendritic_coor_y.values, c='orange')\n",
    "plt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "plt.axis(True)\n",
    "plt.xlabel(\"x (nm)\")\n",
    "plt.ylabel(\"y (nm)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "XY = nx.get_node_attributes(G1, 'XY')\n",
    "plt.scatter(data.dendritic_coor_x.values, data.dendritic_coor_y.values, c='yellow')\n",
    "plt.scatter(linkedDat.dendritic_coor_x.values, linkedDat.dendritic_coor_y.values, c='orange')\n",
    "plt.axis(True)\n",
    "plt.xlabel(\"x (nm)\")\n",
    "plt.ylabel(\"y (nm)\")\n",
    "plt.show()\n",
    "\n",
    "x1 = []\n",
    "y1 = []\n",
    "c1 = []\n",
    "# crazy loop\n",
    "for index, row in linkedDat.iterrows():\n",
    "    x1.append(row.pre_nucleus_x)\n",
    "    x1.append(row.post_nucleus_x)\n",
    "    y1.append(row.pre_nucleus_y)\n",
    "    y1.append(row.post_nucleus_y)\n",
    "    if row.pre_brain_area == \"RL\": c1.append(\"red\")\n",
    "    elif row.pre_brain_area == \"AL\": c1.append(\"blue\")\n",
    "    else: c1.append(\"green\")\n",
    "\n",
    "    if row.post_brain_area == \"RL\": c1.append(\"red\")\n",
    "    elif row.post_brain_area == \"AL\": c1.append(\"blue\")\n",
    "    else: c1.append(\"green\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x1, y1, color=c1)\n",
    "plt.show()\n",
    "# plt.figure()\n",
    "# plt.scatter(x, z, color='pink')\n",
    "# colors = color_nodes(G1)\n",
    "# XZ = nx.get_node_attributes(G1, 'XZ')\n",
    "# # nx.draw(G1, pos=XZ, node_color=colors, node_size=20)\n",
    "# plt.show()\n",
    "\n",
    "allneuron = data2[['pre_nucleus_id', 'pre_nucleus_x', 'pre_nucleus_y', 'pre_nucleus_z',\n",
    "                   'post_nucleus_id', 'post_nucleus_x', 'post_nucleus_y', 'post_nucleus_z']]\n",
    "# allneuronid = np.concatenate([allneuron.pre_nucleus_id.unique(), allneuron.post_nucleus_id.unique()])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c3b5576d3da062"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA & other analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "515e93342a8b5095"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import ast  # Module to safely evaluate literal strings as Python expressions\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "column_to_impute = 'pre_morph_embeddings'\n",
    "\n",
    "data[column_to_impute].fillna(0, inplace=True)\n",
    "\n",
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca02d2602f9c7cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['compartment', 'pre_brain_area','post_brain_area']\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for column in categorical_columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f722ad9df5a356"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculating the Eulidean distances between the axonal and dendritic coordinates\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Euclidean distances and create a new column 'euclidean_distance'\n",
    "data['euclidean_distance'] = np.sqrt(\n",
    "    (data['axonal_coor_x'] - data['dendritic_coor_x'])**2 +\n",
    "    (data['axonal_coor_y'] - data['dendritic_coor_y'])**2 +\n",
    "    (data['axonal_coor_z'] - data['dendritic_coor_z'])**2\n",
    ")\n",
    "\n",
    "# The 'euclidean_distance' column now contains the Euclidean distances\n",
    "\n",
    "# Calculate Euclidean distances between axonal and dendritic coordinates for both pre and post neurons\n",
    "data['axonal_distance_pre'] = np.sqrt((data['axonal_coor_x'] - data['dendritic_coor_x'])**2 + (data['axonal_coor_y'] - data['dendritic_coor_y'])**2)\n",
    "data['axonal_distance_post'] = np.sqrt((data['axonal_coor_x'] - data['dendritic_coor_x'])**2 + (data['axonal_coor_y'] - data['dendritic_coor_y'])**2)\n",
    "\n",
    "data['dendritic_distance_pre'] = np.sqrt((data['dendritic_coor_x'] - data['axonal_coor_x'])**2 + (data['dendritic_coor_y'] - data['axonal_coor_y'])**2)\n",
    "data['dendritic_distance_post'] = np.sqrt((data['dendritic_coor_x'] - data['axonal_coor_x'])**2 + (data['dendritic_coor_y'] - data['axonal_coor_y'])**2)\n",
    "\n",
    "# Combine these distances with Euclidean distances between readout locations\n",
    "data['euclidean_distance_pre'] = np.sqrt((data['pre_rf_x'] - data['post_rf_x'])**2 + (data['pre_rf_y'] - data['post_rf_y'])**2)\n",
    "data['euclidean_distance_post'] = np.sqrt((data['pre_rf_x'] - data['post_rf_x'])**2 + (data['pre_rf_y'] - data['post_rf_y'])**2)\n",
    "\n",
    "# Dataset contains both structural (axonal and dendritic distances) and spatial (Euclidean distances) features.\n",
    "\n",
    "\n",
    "# Calculate the relative positions of post-synaptic neurons with respect to pre-synaptic neurons\n",
    "data['relative_position_x'] = data['post_rf_x'] - data['pre_rf_x']\n",
    "data['relative_position_y'] = data['post_rf_y'] - data['pre_rf_y']\n",
    "\n",
    "# dataset contains \"relative_position_x\" and \"relative_position_y\" features\n",
    "\n",
    "# Create binary directional features\n",
    "data['is_left'] = (data['relative_position_x'] < 0).astype(int)\n",
    "data['is_right'] = (data['relative_position_x'] > 0).astype(int)\n",
    "data['is_above'] = (data['relative_position_y'] < 0).astype(int)\n",
    "data['is_below'] = (data['relative_position_y'] > 0).astype(int)\n",
    "\n",
    "# Dataset contains binary directional features\n",
    "\n",
    "# Create interaction terms between rf_x and rf_y for pre-synaptic neurons\n",
    "data['pre_rf_x_rf_y_interaction'] = data['pre_rf_x'] * data['pre_rf_y']\n",
    "\n",
    "# Create interaction terms between rf_x and rf_y for post-synaptic neurons\n",
    "data['post_rf_x_rf_y_interaction'] = data['post_rf_x'] * data['post_rf_y']\n",
    "\n",
    "# The dataset now contains interaction terms that capture relationships in visual space"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6ccc43bedc3045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Processing the morph embeddings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
    "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
    "\n",
    "def flatten_arrays(arrays):\n",
    "    return [item for sublist in arrays for item in sublist]\n",
    "\n",
    "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(flatten_arrays)\n",
    "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(flatten_arrays)\n",
    "data['pre_morph_embeddings']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c830546b1580886"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA on morph embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all arrays in 'pre_morph_embeddings' and 'post_morph_embeddings' have the same size\n",
    "data['pre_morph_embeddings'] = data['pre_morph_embeddings'].apply(lambda x: np.array(x) if len(x) == 32 else np.array([0.0] * 32))\n",
    "data['post_morph_embeddings'] = data['post_morph_embeddings'].apply(lambda x: np.array(x) if len(x) == 32 else np.array([0.0] * 32))\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data['pre_morph_embeddings'] = list(pca.fit_transform(np.vstack(data['pre_morph_embeddings'])))\n",
    "data['post_morph_embeddings'] = list(pca.fit_transform(np.vstack(data['post_morph_embeddings'])))\n",
    "data['post_morph_embeddings']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f51203b46eee5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA on feature weights\n",
    "scaler = StandardScaler()\n",
    "data['pre_feature_weights'] = data['pre_feature_weights'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
    "data['post_feature_weights'] = data['post_feature_weights'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "def flatten_arrays(arrays):\n",
    "    return [item for sublist in arrays for item in sublist]\n",
    "\n",
    "data['pre_feature_weights'] = data['pre_feature_weights'].apply(flatten_arrays)\n",
    "data['post_feature_weights'] = data['post_feature_weights'].apply(flatten_arrays)\n",
    "\n",
    "\n",
    "# Apply PCApost_feature_weights\n",
    "pca = PCA(n_components=2)\n",
    "data['pre_feature_weights'] = list(pca.fit_transform(np.vstack(data['pre_feature_weights'])))\n",
    "data['post_feature_weights'] = list(pca.fit_transform(np.vstack(data['post_feature_weights'])))\n",
    "\n",
    "\n",
    "\n",
    "# # Extract feature weight columns\n",
    "# feature_weight_columns = ['pre_feature_weights', 'post_feature_weights']\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# data[feature_weight_columns] = scaler.fit_transform(data[feature_weight_columns])\n",
    "\n",
    "# # Create a PCA object\n",
    "# pca = PCA(n_components=2)\n",
    "# data_pca = pca.fit_transform(data[feature_weight_columns])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda49a062c505287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Kurtosis, skew, stats - feature weights\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "data['pre_feature_weights_sum'] = data['pre_feature_weights'].apply(lambda x: sum(x))\n",
    "data['post_feature_weights_sum'] = data['post_feature_weights'].apply(lambda x: sum(x))\n",
    "data['feature_weights_difference'] = abs(data['pre_feature_weights_sum'] - data['post_feature_weights_sum'])\n",
    "\n",
    "data['pre_feature_weights_variance'] = data['pre_feature_weights'].apply(lambda x: np.var(x))\n",
    "data['post_feature_weights_variance'] = data['post_feature_weights'].apply(lambda x: np.var(x))\n",
    "\n",
    "\n",
    "data['pre_feature_weights_skewness'] = data['pre_feature_weights'].apply(lambda x: skew(x))\n",
    "data['post_feature_weights_skewness'] = data['post_feature_weights'].apply(lambda x: skew(x))\n",
    "data['pre_feature_weights_kurtosis'] = data['pre_feature_weights'].apply(lambda x: kurtosis(x))\n",
    "data['post_feature_weights_kurtosis'] = data['post_feature_weights'].apply(lambda x: kurtosis(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7573a582197f473"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Skew, stats, kurt for Morph embeddings\n",
    "from scipy.stats import skew, kurtosis\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sum the morphological embeddings for pre and post neurons\n",
    "data['pre_morph_embeddings_sum'] = data['pre_morph_embeddings'].apply(lambda x: np.sum(x, axis=0))\n",
    "data['post_morph_embeddings_sum'] = data['post_morph_embeddings'].apply(lambda x: np.sum(x, axis=0))\n",
    "\n",
    "# Calculate the Euclidean distance between summed morphological embeddings\n",
    "data['morph_embeddings_distance'] = data.apply(lambda row: np.linalg.norm(row['pre_morph_embeddings_sum'] - row['post_morph_embeddings_sum']), axis=1)\n",
    "\n",
    "# Calculate the variance of morphological embeddings\n",
    "data['pre_morph_embeddings_variance'] = data['pre_morph_embeddings'].apply(lambda x: np.var(x, axis=0))\n",
    "data['post_morph_embeddings_variance'] = data['post_morph_embeddings'].apply(lambda x: np.var(x, axis=0))\n",
    "\n",
    "# Calculate the skewness and kurtosis of morphological embeddings\n",
    "data['pre_morph_embeddings_skewness'] = data['pre_morph_embeddings'].apply(lambda x: skew(x, axis=0))\n",
    "data['post_morph_embeddings_skewness'] = data['post_morph_embeddings'].apply(lambda x: skew(x, axis=0))\n",
    "data['pre_morph_embeddings_kurtosis'] = data['pre_morph_embeddings'].apply(lambda x: kurtosis(x, axis=0))\n",
    "data['post_morph_embeddings_kurtosis'] = data['post_morph_embeddings'].apply(lambda x: kurtosis(x, axis=0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5d9302afdd323"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA with Explained variance on whole data set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Exclude non-numeric and non-categorical columns from PCA\n",
    "exclude_cols = ['ID',\n",
    "                'pre_feature_weights',\n",
    "                'post_feature_weights',\n",
    "                'pre_morph_embeddings',\n",
    "                'post_morph_embeddings',\n",
    "                'projection_group',\n",
    "                'connected',\n",
    "                'is_left',\n",
    "                'is_right',\n",
    "                'is_above',\n",
    "                'is_below'\n",
    "               ]\n",
    "\n",
    "# Select only numeric columns for PCA\n",
    "numeric_columns = [col for col in data.columns if col not in exclude_cols]\n",
    "\n",
    "# Standardize the data (if needed)\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "pca_result = pca.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = explained_variance.cumsum()\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-', color='b')\n",
    "plt.title('Explained Variance with PCA')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a99dd12ff3724f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA to data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Exclude non-numeric and non-categorical columns from the PCA\n",
    "exclude_cols = ['ID',\n",
    "                'pre_feature_weights',\n",
    "                'post_feature_weights',\n",
    "                'pre_morph_embeddings',\n",
    "                'post_morph_embeddings',\n",
    "                'projection_group',\n",
    "                'connected',\n",
    "                'is_left',\n",
    "                'is_right',\n",
    "                'is_above',\n",
    "                'is_below'\n",
    "               ]\n",
    "\n",
    "# Select only numeric columns for PCA\n",
    "numeric_columns = [col for col in data.columns if col not in exclude_cols]\n",
    "pca_num_of_components = 30\n",
    "\n",
    "# Standardize the data (if needed)\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Create a PCA object (you can specify the number of components)\n",
    "pca = PCA(n_components=pca_num_of_components)\n",
    "\n",
    "# Fit and transform the data\n",
    "pca_result = pca.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[col for col in range(pca_num_of_components)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3267117d74bc8a1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K means clustering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fca5b809842e3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select only numeric columns for KMeans clustering\n",
    "numeric_columns_for_clustering = [col for col in pca_df.columns if col != 'Cluster']\n",
    "\n",
    "num_clusters = 2\n",
    "\n",
    "# Create a KMeans object\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the model to the PCA results\n",
    "kmeans.fit(pca_df[numeric_columns_for_clustering])\n",
    "\n",
    "# Add the cluster labels to the PCA DataFrame\n",
    "pca_df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters in the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = pca_df[pca_df['Cluster'] == cluster]\n",
    "    plt.scatter(cluster_data[0], cluster_data[1], label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "plt.title('K-means Clustering after PCA')\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a4f49418093457"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ff4b18be336a11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Datamanager: handle the input data and prepare all preprocessing for selected DNN model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a10779794e28076a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"\n",
    "    Handle DNN dataset and apply modification needed\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._dat_main = None\n",
    "        self._dat_lboard = None\n",
    "        self._dat_feat = None\n",
    "        self._dat_morp = None\n",
    "        self.BATCH_SIZE = 0\n",
    "        self.EPOCHS = 0\n",
    "        self.BUFFER_SIZE = 0\n",
    "        self._train_ds = None\n",
    "        self._val_ds = None\n",
    "        self._test_ds = None\n",
    "        self.step_per_epoch = None\n",
    "        self.lea_ds = None  # wip. just a bunch of feature for now\n",
    "        self.pre_morph_pca = None\n",
    "        self.post_morph_pca = None\n",
    "\n",
    "    def config(self, batch_size: int, epochs: int, buffer_size: int):\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.EPOCHS = epochs\n",
    "        self.BUFFER_SIZE = buffer_size\n",
    "\n",
    "    def load_dat_feat(self, fname: str):\n",
    "        self._dat_feat = pd.read_csv(fname)\n",
    "        # join all morph_embed_i columns into a single np.array column\n",
    "        self._dat_feat[\"feature_weights\"] = (\n",
    "            self._dat_feat.filter(regex=\"feature_weight_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        self._dat_feat.drop(self._dat_feat.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True)\n",
    "\n",
    "    def load_dat_morp(self, fname: str):\n",
    "        self._dat_morp = pd.read_csv(fname)\n",
    "        self._dat_morp[\"morph_embeddings\"] = (\n",
    "            self._dat_morp.filter(regex=\"morph_emb_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        self._dat_morp.drop(self._dat_morp.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True)\n",
    "\n",
    "    def load_dat_main(self, fname: str):\n",
    "        if not self.dataIsLoadedDat():\n",
    "            raise Exception(\"please load morp and feature data first\")\n",
    "        self._dat_main = pd.read_csv(fname)\n",
    "\n",
    "        self._dat_main = (\n",
    "            self._dat_main.merge(\n",
    "                self._dat_feat.rename(columns=lambda x: \"pre_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_feat.rename(columns=lambda x: \"post_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_morp.rename(columns=lambda x: \"pre_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_morp.rename(columns=lambda x: \"post_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dm1, dm2 = len(self._dat_main['pre_morph_embeddings'][0]), len(self._dat_main['post_morph_embeddings'][0])\n",
    "        for i, r in self._dat_main.iterrows():\n",
    "            if isinstance(r['pre_morph_embeddings'], float):\n",
    "                self._dat_main.at[i, 'pre_morph_embeddings'] = np.full(dm1, 0.001)\n",
    "            if isinstance(r['post_morph_embeddings'], float):\n",
    "                self._dat_main.at[i, 'post_morph_embeddings'] = np.full(dm2, 0.001)\n",
    "\n",
    "    def load_dat_lboard(self, fname: str):\n",
    "        if not self.dataIsLoadedDat():\n",
    "            raise Exception(\"please load morp and feature data first\")\n",
    "\n",
    "        self._dat_lboard = pd.read_csv(fname)\n",
    "\n",
    "        self._dat_lboard = (\n",
    "            self._dat_lboard.merge(\n",
    "                self._dat_feat.rename(columns=lambda x: \"pre_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_feat.rename(columns=lambda x: \"post_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_morp.rename(columns=lambda x: \"pre_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "            .merge(\n",
    "                self._dat_morp.rename(columns=lambda x: \"post_\" + x),\n",
    "                how=\"left\",\n",
    "                validate=\"m:1\",\n",
    "                copy=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dm1, dm2 = len(self._dat_lboard['pre_morph_embeddings'][0]), len(self._dat_lboard['post_morph_embeddings'][0])\n",
    "        for i, r in self._dat_lboard.iterrows():\n",
    "            if isinstance(r['pre_morph_embeddings'], float):\n",
    "                self._dat_lboard.at[i, 'pre_morph_embeddings'] = np.full(dm1, 0.001)\n",
    "            if isinstance(r['post_morph_embeddings'], float):\n",
    "                self._dat_lboard.at[i, 'post_morph_embeddings'] = np.full(dm2, 0.001)\n",
    "\n",
    "    def dataIsLoadedDat(self):\n",
    "        return not (self._dat_morp is None or self._dat_feat is None)\n",
    "\n",
    "    def prepTrainValTest(self, w_trn, w_val, w_tst, random_state=0):\n",
    "        # split data\n",
    "        tmp = w_tst/(w_trn+w_val+w_tst)\n",
    "        dm_df, test_df = train_test_split(self._dat_main, test_size=tmp, random_state=random_state)\n",
    "        tmp = w_val/(w_val+w_trn)\n",
    "        train_df, val_df = train_test_split(dm_df, test_size=tmp, random_state=random_state)\n",
    "\n",
    "        # process\n",
    "        f_trn, l_trn = self.processFromDf(train_df, trainmode=True)\n",
    "        f_val, l_val = self.processFromDf(val_df, trainmode=False)\n",
    "        f_tst, l_tst = self.processFromDf(test_df, trainmode=False)\n",
    "\n",
    "        # split positive and negative sample and make them\n",
    "        pos_ds = self.makeDs(f_trn, l_trn, np.where(l_trn == 1))\n",
    "        neg_ds = self.makeDs(f_trn, l_trn, np.where(l_trn == 0))\n",
    "        resampled_ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[.5, .5])\n",
    "\n",
    "        self._train_ds = resampled_ds.batch(self.BATCH_SIZE).prefetch(2)\n",
    "        # self._train_ds = self.makeDs(f_trn, l_trn)  # test without rebalancing\n",
    "        # self._train_ds = pos_ds.batch(self.BATCH_SIZE).prefetch(2)  # train with just positive to offset the balance\n",
    "        self.step_per_epoch = np.ceil(2.0 * (len(l_trn) - np.count_nonzero(l_trn)) / self.BATCH_SIZE)\n",
    "\n",
    "        self._val_ds = self.makeDs(f_val, l_val)\n",
    "        self._test_ds = self.makeDs(f_tst, l_tst)\n",
    "\n",
    "    def loadLeader(self):\n",
    "        self.lea_ds, _ = self.processFromDf(self._dat_lboard, skip_y=1, trainmode=False)\n",
    "\n",
    "    def getTrainDs(self):\n",
    "        return self._train_ds\n",
    "    \n",
    "    def getValDs(self):\n",
    "        return self._val_ds\n",
    "    \n",
    "    def getTestDs(self):\n",
    "        return self._test_ds\n",
    "\n",
    "    def getLeadDS(self):\n",
    "        return self.lea_ds\n",
    "\n",
    "    @staticmethod\n",
    "    def oneHotConverter(raw):\n",
    "        \"\"\"\n",
    "        convert a 1D array to one-hot-coding\n",
    "        :param raw: an array\n",
    "        :return: 2D matrix\n",
    "        \"\"\"\n",
    "        cls = np.unique(raw)\n",
    "        out = np.zeros([len(raw), len(cls)])\n",
    "        cls_look_up = {item: index for index, item in enumerate(cls)}  # convert array to dict\n",
    "        for c, i in enumerate(raw):\n",
    "            out[c, cls_look_up[i]] = 1  # mark target code to 1\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def cosin_feature_similarity(df, feat1, feat2):\n",
    "        # perform cosin similarity on 2 feature and return array\n",
    "        pre = np.vstack(df[feat1].values)\n",
    "        post = np.vstack(df[feat2].values)\n",
    "        out = np.zeros(len(pre))\n",
    "        for c, _ in enumerate(out):\n",
    "            out[c] = np.dot(pre[c], post[c])/(np.linalg.norm(pre[c]) * np.linalg.norm(post[c]))\n",
    "        return out\n",
    "\n",
    "    def processFromDf(self, raw_df, skip_y=0, trainmode=True):\n",
    "        # engineering features\n",
    "        raw_df['fweight_cosin'] = DataManager.cosin_feature_similarity(\n",
    "            raw_df, 'pre_feature_weights', 'post_feature_weights')\n",
    "        raw_df['morp_cosin'] = DataManager.cosin_feature_similarity(\n",
    "            raw_df, 'pre_morph_embeddings', 'post_morph_embeddings')\n",
    "\n",
    "        raw_df['rf_dist'] = np.linalg.norm(raw_df[['pre_rf_x', 'pre_rf_y']].values -\n",
    "                                           raw_df[['post_rf_x', 'post_rf_y']].values, axis=1)\n",
    "\n",
    "        raw_df['neural_displacement_x'] = (raw_df.post_nucleus_x - raw_df.pre_nucleus_x) / 100000\n",
    "        raw_df['neural_displacement_y'] = (raw_df.post_nucleus_y - raw_df.pre_nucleus_y) / 100000\n",
    "        raw_df['neural_displacement_z'] = (raw_df.post_nucleus_z - raw_df.pre_nucleus_z) / 100000\n",
    "\n",
    "\n",
    "        indat1 = np.copy(raw_df[['adp_dist', 'pre_oracle', 'post_oracle', 'pre_skeletal_distance_to_soma',\n",
    "                                 'post_skeletal_distance_to_soma', 'pre_test_score', 'post_test_score', 'fweight_cosin',\n",
    "                                 'morp_cosin', 'rf_dist', 'neural_displacement_x', 'neural_displacement_y',\n",
    "                                 'neural_displacement_z']].values)\n",
    "        # brain area\n",
    "        raw = raw_df.pre_brain_area.values\n",
    "        indat2 = DataManager.oneHotConverter(raw)\n",
    "        raw = raw_df.post_brain_area.values\n",
    "        indat3 = DataManager.oneHotConverter(raw)\n",
    "        # compartment\n",
    "        raw = raw_df.compartment.values\n",
    "        indat4 = DataManager.oneHotConverter(raw)\n",
    "\n",
    "        # morph embedding\n",
    "        indat5 = np.vstack(raw_df.pre_morph_embeddings.values)\n",
    "        indat6 = np.vstack(raw_df.post_morph_embeddings.values)\n",
    "        # if trainmode:\n",
    "        #     pca = PCA(n_components=8)\n",
    "        #     indat5 = pca.fit_transform(np.vstack(raw_df.pre_morph_embeddings))\n",
    "        #     self.pre_morph_pca = pca.components_\n",
    "        #     indat6 = pca.fit_transform(np.vstack(raw_df.post_morph_embeddings))\n",
    "        #     self.post_morph_pca = pca.components_\n",
    "        # else:\n",
    "        #     indat5 = np.dot(np.vstack(raw_df.pre_morph_embeddings), self.pre_morph_pca.T)\n",
    "        #     indat6 = np.dot(np.vstack(raw_df.post_morph_embeddings), self.post_morph_pca.T)\n",
    "\n",
    "        # feature weight\n",
    "        # indat7 = np.stack(raw_df.pre_feature_weights)\n",
    "        # indat8 = np.stack(raw_df.post_feature_weights)\n",
    "\n",
    "        # indat7 = pca_pre_feature_weight_result\n",
    "        # indat8 = pca_post_feature_weight_result\n",
    "\n",
    "        labels = None\n",
    "        if not skip_y:\n",
    "            raw = raw_df.connected.values\n",
    "            labels = DataManager.oneHotConverter(raw)[:, 1]\n",
    "\n",
    "        # Normalization\n",
    "        # transform bulk data  -- try forcing to magnitude of 10\n",
    "        arr = np.transpose(indat1)\n",
    "        arr[0] = arr[0] / 5000  # scale down large value\n",
    "        # arr[1] = arr[1] * 10  # upscale\n",
    "        # arr[2] = arr[2] * 10  # upscale\n",
    "        arr[3] = arr[3] / 2000000  # scale down large value\n",
    "        arr[4] = arr[4] / 2000000  # scale down large value\n",
    "        arr[9] = arr[9] / 30\n",
    "        return (indat1, indat2, indat3, indat4, indat5, indat6), labels\n",
    "\n",
    "    def makeDs(self, features_list, labels, index=None):\n",
    "        \"\"\"\n",
    "        :param features_list: [feature1, feature2, ...]\n",
    "        :param labels: output label\n",
    "        :param index: position for this dataset -- training need index to split pos and neg sample for rabalancing, \n",
    "        val and test does not. I know it's wrong in every shape and form, but I'm lazy\n",
    "        :return: tf.dataset\n",
    "        \"\"\"\n",
    "        used_features = {}\n",
    "        ds = None\n",
    "        if index is None:\n",
    "            for c, feature in enumerate(features_list):\n",
    "                used_features[\"Feature {}\".format(c)] = feature\n",
    "            ds = tf.data.Dataset.from_tensor_slices((used_features, labels)).cache()\n",
    "            ds = ds.batch(self.BATCH_SIZE).prefetch(2)\n",
    "        else:\n",
    "            for c, feature in enumerate(features_list):\n",
    "                used_features[\"Feature {}\".format(c)] = feature[index]\n",
    "            ds = tf.data.Dataset.from_tensor_slices((used_features, labels[index]))\n",
    "            ds = ds.shuffle(self.BUFFER_SIZE).repeat()\n",
    "        return ds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f007f8dbb6c564c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NeroClassifier: DNN model used to detect synapses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "102936644b9b5c50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from DataManager import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "SEED = 7\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "BUFFER_SIZE = 10000\n",
    "STEP_PER_EPOCH = 32\n",
    "TRN = 0.8\n",
    "VAL = 0.1\n",
    "TST = 0.1\n",
    "\n",
    "METRICS = [\n",
    "    keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "    keras.metrics.MeanSquaredError(name='MSE'),\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR'),  # precision-recall curve\n",
    "]\n",
    "checkpoint_path_store = 'resource/checkpoint/dum.ckpt'\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', verbose=1, patience=10, mode='max',\n",
    "                                                  restore_best_weights=True)\n",
    "keras.utils.set_random_seed(SEED)\n",
    "# activate GPU accelerator\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')  # Use the first GPU\n",
    "\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_store, save_weights_only=True, save_freq=5)\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\", \" \").capitalize()\n",
    "        plt.subplot(2, 2, n + 1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_' + metric],\n",
    "                 color=colors[1], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8, 1])\n",
    "        else:\n",
    "            plt.ylim([0, 1])\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "# Input Data\n",
    "datamanager = DataManager()\n",
    "datamanager.config(BATCH_SIZE, EPOCHS, BUFFER_SIZE)\n",
    "datamanager.load_dat_feat(\"./resource/feature_weights.csv\")\n",
    "datamanager.load_dat_morp(\"./resource/morph_embeddings.csv\")\n",
    "datamanager.load_dat_main(\"./resource/train_data2.csv\")\n",
    "datamanager.prepTrainValTest(TRN, VAL, TST, SEED)\n",
    "# Create a model\n",
    "in0 = keras.layers.Input(shape=(13,), dtype='float32', name='Feature 0')\n",
    "in1 = keras.layers.Input(shape=(3,), dtype='float32', name='Feature 1')\n",
    "in2 = keras.layers.Input(shape=(3,), dtype='float32', name='Feature 2')\n",
    "in3 = keras.layers.Input(shape=(7,), dtype='float32', name='Feature 3')\n",
    "in4 = keras.layers.Input(shape=(32,), dtype='float32', name='Feature 4')\n",
    "in5 = keras.layers.Input(shape=(32,), dtype='float32', name='Feature 5')\n",
    "# in6 = keras.layers.Input(shape=(512,), dtype='float32', name='Feature 6')\n",
    "# in7 = keras.layers.Input(shape=(512,), dtype='float32', name=\"Feature 7\")\n",
    "layer = keras.layers.Concatenate(axis=1)([in0, in1, in2, in3, in4, in5])\n",
    "layer = keras.layers.Dense(64, activation=keras.activations.gelu)(layer)\n",
    "layer = keras.layers.Dropout(0.25)(layer)\n",
    "layer = keras.layers.Dense(64, activation=keras.activations.gelu)(layer)\n",
    "layer = keras.layers.Dropout(0.25)(layer)\n",
    "layer = keras.layers.Dense(32, activation=keras.activations.gelu)(layer)\n",
    "# drop3 = keras.layers.Dropout(0.2)(dense3)\n",
    "output = keras.layers.Dense(1, activation=keras.activations.sigmoid)(layer)\n",
    "model = keras.models.Model([in0, in1, in2, in3, in4, in5], output)\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)  # Set the constant learning rate\n",
    "loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.5, gamma=8.0)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=METRICS)  # Using binary cross-entropy loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e217845d16ac37a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load store weight if want to continue\n",
    "checkpoint_path_load = 'resource/checkpoint/smater5.ckpt'\n",
    "# model.load_weights(checkpoint_path_load)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a67363601db1997"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    datamanager.getTrainDs(),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEP_PER_EPOCH,\n",
    "    # steps_per_epoch=datamanager.step_per_epoch,\n",
    "    callbacks=[early_stopping, cp_callback],\n",
    "    validation_data=(datamanager.getValDs()))\n",
    "plot_metrics(history)\n",
    "\n",
    "hist_val = model.evaluate(datamanager.getValDs())\n",
    "print(\"[{} {}]\\n[{} {}]\".format(hist_val[3], hist_val[4], hist_val[5], hist_val[6]))\n",
    "print((hist_val[3] / (hist_val[3] + hist_val[6]) + hist_val[5] / (hist_val[5] + hist_val[4])) / 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b6e53765b30c519"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only run this when you're ready for test\n",
    "hist_test = model.evaluate(datamanager.getTestDs())\n",
    "print(\"[{} {}]\\n[{} {}]\".format(hist_test[3], hist_test[4], hist_test[5], hist_test[6]))\n",
    "print((hist_test[3] / (hist_test[3] + hist_test[6]) + hist_test[5] / (hist_test[5] + hist_test[4])) / 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b2cfc0ee04805d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# leaderboard\n",
    "datamanager.load_dat_lboard('./resource/leaderboard_data.csv')\n",
    "datamanager.loadLeader()\n",
    "pred = model.predict(datamanager.lea_ds)\n",
    "lead = datamanager._dat_lboard\n",
    "lead['connected'] = pred >= 0.5\n",
    "submission_data = lead.filter(['ID', 'connected'])\n",
    "submission_data.to_csv('submission_data_dnn_report3.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7acf52f16aae1d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_true = np.concatenate([y.numpy() for x, y in datamanager.getTestDs()], axis=0)\n",
    "# y_prop = model.predict(datamanager.getTestDs())\n",
    "# dum = []\n",
    "# dumx = []\n",
    "# for i in range(20, 80):\n",
    "#     dumx.append(i/100)\n",
    "#     y_pred = (y_prop >= i/100).astype(float)\n",
    "#     dum.append(balanced_accuracy_score(y_true, y_pred))\n",
    "# plt.figure()\n",
    "# plt.plot(dumx, dum)\n",
    "# plt.show()\n",
    "# \n",
    "# # some stupid test\n",
    "# a = np.array([])\n",
    "# b = np.array([])\n",
    "# a = a == 1\n",
    "# thres = 0.6\n",
    "# \n",
    "# confusion_matrix(a, b>thres)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4488733b35cdaf6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
